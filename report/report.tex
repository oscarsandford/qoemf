% Based on the template for ICIP-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


\title{On Using Mininet to Evaluate QoS for Node.js Back Ends}
\name{Juan Flores, Oscar Sandford, and Ben Wunderlich
\thanks{Thank you to Dr. Jianping Pan at the University of Victoria for his teaching and guidance throughout this project and the course.}}
\address{University of Victoria}
\begin{document}
%
\maketitle
%
\begin{abstract}
Write this last.
\end{abstract}
%
\begin{keywords}
Quality of Service, Software Defined Network, Network emulation, Mininet, Node.js
\end{keywords}
%
% Overview of the problem. Why is it important?
\section{Introduction}
The design of computer networks is driven by a drive towards a target quality of service (QoS), a set of performance measurements. Further, the implementation of 
network architectures demands time and money, and implementations that fail to meet quality of service standards waste time for both companies and customers. 
Software-defined networks (SDN) addresses the project of static architectures by enabling programming and measurement in a dynamic setting. It is important to understand 
and harness the strengths of SDN in a methodical way, so developers can focus more on the design goals and less on tedious implementation tasks. 

There does not yet exist a qualitative evaluation of such measurement techniques for modern web servers such as Node.js. Providing a survey of various QoS measurement 
metrics for this domain will enable web developers to better understand and optimize the QoS metrics that matter most to their application domain.

% What has already been done? Why are their efforts not sufficient?
\section{Related Work}
Previous studies have defined quality of services measurements \cite{qos_analysis_2022} and used network emulation tools to program their own experiments 
\cite{chauhan_atulkar_2020}. The metrics presented in \cite{qos_analysis_2022} are throughput, delay, packet loss, and jitter. The authors used these metrics in a 
generic setting (i.e. no specific domain) in order to compare TCP and UDP protocols. While this domain-agnostic approach makes for a good start, there are several 
flaws in their results. Firstly, no packet loss was experienced, and therefore the measurement was ignored. Secondly, previous studies used round trip time (RTT) 
as a QoS metric, which \cite{qos_analysis_2022} did not consider. 

Another study that investigated load balancing algorithms looked at throughput, response time, and memory utilization \cite{babbar_2022}. Measuring memory utilization 
is effective for load balancing, but we are considering quality of service, with the focus on the end user. The weight on the server only matters if it affects the 
client. This paper enforces throughput as a necessary metric, and that response time (or RTT) must be considered. 

Regencia and Yu's paper develops a completely configurable QoS testing framework \cite{regencia_2021}. Their framework allows for topology, switch, load, host, and 
other such configuration, but this configurablility add unnecessary overhead to our specific problem. 

A different paper \cite{AweQoS_2020} proposes a framework titled AweQoS which combines both Quality of Service tests with security tests.    
The framework includes bandwitdth speed measurement, delay tests and jitter tests for the QoS and SYN flood test, UDP flood test and Slow HTTP flood test. 
This paper shows promise in being able to combine different tests in-order to accurately measure the performance of a network. Despite this, the only in-depth outcome 
was a comperative band-witdh test between iPerf and the proposed AweQos bandwidth test using UDP. It furthers the idea that QoS type frameworks may have potnetial but 
have not been thouroughly tested enough.

% What is our approach? Why can it do better or differently?
\section{Approach}
Mininet is utilized as a network emulator, for it has seen significant use in related works \cite{mininet_emulation_2014,qos_analysis_2022,chauhan_atulkar_2020} and 
is well-documented. The Iperf tool was used in \cite{chauhan_atulkar_2020} for generating network traffic, and its use is considered for the same purposes in this 
study. We reused the quality of service metrics outlined in \cite{qos_analysis_2022}: 
\begin{itemize}
    \item Throughput (total transmitted data in bits)/(total time taken in seconds)
    \item Delay (time required to transmit the data from sender to receiver)
    \item Packet loss (the number of packets not delivered to their destination)
    \item Jitter (the variance in latency)
\end{itemize} 

Mininet's toolkit is used to create a server around a simple Node.js application, and artificially adjust flow control, packet drop rate, and jitter from the server-side. 
Mininet measures packet return trip time (RTT), delay, and throughput from the server to clients. The individual results of these measurements determine how the overall 
quality of service changes as various server-side features are tweaked dynamically. Additionally, we show how to use Mininet to simulate DoS attacks by a large number 
of clients in order to observe how attacks affect quality of service to benign users.

The qualitative effects of the QoS metrics we used are analyzed to see which metrics are best for the domain of this problem. As a result of this study, we provide a 
quality of service measurement outline for future developers to consider when testing their Node.js applications in a pseudo-live setting using Mininet. 

Our project is extensible and configurable. All network simulation is done on Mininet's pre-built Ubuntu virtual machine image \cite{mininet_github_2021}, and our 
source code for running Mininet in Python with a Node.js server is provided at the end of the work. 

% Expected deliverables and a rough biweekly time schedule.
\subsection{Timeline}
\begin{enumerate}
    \item Jan 31st - Feb 13th: Create project outline, gather resources, outline program and create git repository.
    \item Feb 14th - Feb 27th: Build testing environment and prepare test.
    \item Feb 28th - Mar 13th: Begin tests, create midterm presentation and present. 
    \item Mar 14th - Mar 27th: Gather results and prepare final presentation. Begin final report.
    \item Mar 28th - Apr 7th: Final Presentation. Continue to work on final report.
    \item Apr 8th - Apr 16th: Turn in final report.
\end{enumerate}

\subsection{Project Website}
Please see the associated website for progress reports and results at https://oscarsandford.github.io/qoemf/.

% Some plots and stuff here about what we did.
\section{Experiments}
Text here.

% Discussion about the results.
\section{Discussion}
Text here.

% Wrap up and encourage further research in QoS.
\section{Conclusion}
Text here.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
